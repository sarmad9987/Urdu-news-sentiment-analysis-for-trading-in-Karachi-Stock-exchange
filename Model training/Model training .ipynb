{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(len(tf.config.list_physical_devices('GPU'))>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkGp_bVq7u71",
    "outputId": "9b092b91-6e19-4503-9724-8d1322849aa5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "!pip install urduhack[tf-gpu]\n",
    "import urduhack\n",
    "urduhack.download()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASja-DEjxqVt",
    "outputId": "27c76ebd-8d90-4cba-fbe8-4f42c3efe168"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T0mFJsz8qw_"
   },
   "outputs": [],
   "source": [
    "urdu_news= pd.read_csv('Data Labels.csv')\n",
    "df= urdu_news.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kH_Cr4hiDLdg",
    "outputId": "2a82e9d4-5796-4450-a69b-5f0304a8e63f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "5NLGRGTn802d",
    "outputId": "847e13ca-8f66-4c3c-f6bb-c1a2f5b662c4"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Class', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "kUs-h_QvCbhs",
    "outputId": "5dea0399-460c-489d-89a4-d218054873a9"
   },
   "outputs": [],
   "source": [
    "temp = df.groupby('Class').count()['labels'].reset_index().sort_values(by='Class',ascending=True)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "XKGrnWIvBM9L",
    "outputId": "8a39573c-d9b4-48e3-c690-25d7e93a187a"
   },
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "fig = go.Figure(go.Funnelarea(\n",
    "    text =temp.Class,\n",
    "    values = temp.labels,\n",
    "    title = {\"position\": \"top center\", \"text\": \"Sentiment Class Distribution\"}\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bKstPfIh84qd",
    "outputId": "c7b08fb9-4669-48fe-e54d-a73d60a00149"
   },
   "outputs": [],
   "source": [
    "df['Date']=df['Date'].str.replace('/', '-')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBLabPto84tD"
   },
   "outputs": [],
   "source": [
    "#urduhack.download()\n",
    "import urduhack\n",
    "from urduhack.normalization import normalize, normalize_characters, normalize_combine_characters, remove_diacritics\n",
    "from urduhack.preprocessing import preprocess, remove_punctuation, remove_accents, replace_urls, replace_emails, replace_numbers, replace_currency_symbols,digits_space, normalize_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1ewMgDp84vV"
   },
   "outputs": [],
   "source": [
    "# Text Normalization\n",
    "df['News Headlines'] = df['News Headlines'].apply(normalize)\n",
    "\n",
    "# Text normalize characters\n",
    "df['News Headlines'] = df['News Headlines'].apply(normalize_characters)\n",
    "\n",
    "# Text normalize combined characters\n",
    "df['News Headlines'] = df['News Headlines'].apply(normalize_combine_characters)\n",
    "\n",
    "# remove_diacritics\n",
    "df['News Headlines'] = df['News Headlines'].apply(remove_diacritics)\n",
    "\n",
    "df['News Headlines'] = df['News Headlines'].apply(normalize_whitespace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnBN1nQG84xj"
   },
   "outputs": [],
   "source": [
    "df['News Headlines'] = df['News Headlines'].apply(preprocess) \n",
    "df['News Headlines'] = df['News Headlines'].apply(remove_punctuation) \n",
    "df['News Headlines'] = df['News Headlines'].apply(remove_accents)\n",
    "df['News Headlines'] = df['News Headlines'].apply(replace_urls) \n",
    "df['News Headlines'] = df['News Headlines'].apply(replace_numbers) \n",
    "df['News Headlines'] = df['News Headlines'].apply(replace_emails) \n",
    "df['News Headlines'] = df['News Headlines'].apply(replace_currency_symbols) \n",
    "df['News Headlines'] = df['News Headlines'].apply(normalize_whitespace) \n",
    "df['News Headlines'] = df['News Headlines'].apply(digits_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR0KPPFUqUiL"
   },
   "source": [
    "stop_words=\"\"\"آ آئی آئیں آئے آتا آتی آتے آداب آدھ آدھا آدھی آدھے آس\n",
    " آمدید آنا آنسہ آنی آنے آپ آگے آہ آہا آیا اب ابھی ابے\n",
    " اتوار ارب اربویں ارے اس اسکا اسکی اسکے اسی اسے اف افوہ الاول البتہ\n",
    " الثانی الحرام السلام الف المکرم ان اندر انکا انکی انکے انہوں انہی انہیں\n",
    " اوئے اور اوپر اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکبر اکثر اگر اگرچہ\n",
    " اگست اہاہا ایسا ایسی ایسے ایک بائیں بار بارے بالکل باوجود باہر بج بجے\n",
    " بخیر برسات بشرطیکہ بعض بغیر بلکہ بن بنا بناؤ بند بڑی بھر بھریں\n",
    " بھی بہار بہت بہتر بیگم تاکہ تاہم تب تجھ تجھی تجھے ترا تری\n",
    " تلک تم تمام تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے\n",
    " تہائی تیرا تیری تیرے تین جا جاؤ جائیں جائے جاتا جاتی جاتے جانی جانے\n",
    " جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی جیسا\n",
    " جیسوں جیسی جیسے جیٹھ حالانکہ حالاں حصہ حضرت خاطر خالی خدا خزاں خواہ خوب\n",
    " خود دائیں درمیان دریں دو دوران دوسرا دوسروں دوسری دوشنبہ دوں دکھائیں دگنا دی\n",
    " دیئے دیا دیتا دیتی دیتے دیر دینا دینی دینے دیکھو دیں دیے دے ذریعے\n",
    " رکھا رکھتا رکھتی رکھتے رکھنا رکھنی رکھنے رکھو رکھی رکھے رہ رہا رہتا\n",
    " رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی\n",
    " سراسر سلام سمیت سوا سوائے سکا سکتا سکتے سہ سہی سی سے شام شاید\n",
    " شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور علاوہ عین فروری فقط فلاں\n",
    " فی قبل قطا لائی لائے لاتا لاتی لاتے لانا لانی لایا لو لوجی لوگوں\n",
    " لگ لگا لگتا لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن\n",
    " لیں لیے لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محترمی محض مرا مرحبا\n",
    " مری مرے مزید مس مسز مسٹر مطابق مطلق مل منٹ منٹوں مکرمی مگر\n",
    " مگھر مہربانی میرا میروں میری میرے میں نا نزدیک نما نو نومبر نہ نہیں\n",
    " نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ\n",
    " وعلیکم وغیرہ ولے وگرنہ وہ وہاں وہی وہیں ویسا ویسے ویں پاس\n",
    " پایا پر پس پلیز پون پونا پونی پونے پھاگن پھر پہ پہر پہلا پہلی\n",
    " پہلے پیر پیچھے چاہئے چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ\n",
    " چوگنی چکی چکیں چکے چہارشنبہ چیت ڈالنی ڈالنے ڈالے کئے کا کاتک کاش کب\n",
    " کبھی کدھر کر کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس\n",
    " کسی کسے کل کم کن کنہیں کو کوئی کون کونسا کونسے کچھ کہ کہا\n",
    " کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے کے\n",
    " گئی گئے گا گرما گرمی گنا گو گویا گھنٹا گھنٹوں گھنٹے گی گیا\n",
    " ہائیں ہائے ہاڑ ہاں ہر ہرچند ہرگز ہزار ہفتہ ہم ہمارا ہماری ہمارے ہمی\n",
    " ہمیں ہو ہوئی ہوئیں ہوئے ہوا ہوبہو ہوتا ہوتی ہوتیں ہوتے ہونا ہونگے ہونی\n",
    " ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj7hfUhe9bB3"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join(word for word in text.split() if word not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcFzClGK9bFL"
   },
   "outputs": [],
   "source": [
    "#df['News Headlines'] =  df['News Headlines'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEXwKm179bIy"
   },
   "outputs": [],
   "source": [
    "from urduhack.models.lemmatizer import lemmatizer\n",
    "def lemitizeStr(str):\n",
    "    lemme_str = \"\"\n",
    "    temp = lemmatizer.lemma_lookup(str)\n",
    "    for t in temp:\n",
    "        lemme_str += t[0] + \" \"\n",
    "    \n",
    "    return lemme_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN_88FQ39bLa"
   },
   "outputs": [],
   "source": [
    "df['lemmatized_text'] = df['News Headlines'].apply(lemitizeStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Sa0OX5MQ_f"
   },
   "source": [
    "## **Split Dataset into Train,Test and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1,random_state=42)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBIx0G0GMEgK",
    "outputId": "8ebe1b72-ffa7-473f-f4f7-a30878970268"
   },
   "outputs": [],
   "source": [
    "split_index_1 = int(len(df) * 0.8)\n",
    "split_index_2 = int(len(df) * 0.9)\n",
    "\n",
    "train_df1, val_df1, test_df1 = df[:split_index_1], df[split_index_1:split_index_2], df[split_index_2:]\n",
    "\n",
    "len(train_df1), len(val_df1), len(test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMWtjNlP405c"
   },
   "outputs": [],
   "source": [
    "train_df1.rename(str.lower, axis='columns',inplace=True)\n",
    "test_df1.rename(str.lower, axis='columns',inplace=True)\n",
    "val_df1.rename(str.lower, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an_tP-sdMEnb",
    "outputId": "bc118f72-9c65-446f-bf49-6f942e48911c"
   },
   "outputs": [],
   "source": [
    "print(df.shape) \n",
    "print(train_df1.shape) \n",
    "print(val_df1.shape)\n",
    "print(test_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpYjROKvfLpp",
    "outputId": "2532396a-5d1e-44b3-b744-60af605c1261"
   },
   "outputs": [],
   "source": [
    "train_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "tLVXNDMkMEp8",
    "outputId": "5c0a9016-7556-41ab-a4ef-7d07856bddc5"
   },
   "outputs": [],
   "source": [
    "sns.countplot (x='class',data=train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "K1-OW9RaMCIU",
    "outputId": "1451f1e2-916c-4235-9367-ec2e689950af"
   },
   "outputs": [],
   "source": [
    "sns.countplot (x='class',data=test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "SUj2SNCMMCLN",
    "outputId": "a4490200-4d78-4052-9fee-dea346cbf952"
   },
   "outputs": [],
   "source": [
    "sns.countplot (x='class',data=val_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLazTEmz5oM3"
   },
   "source": [
    "### **Language Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpcYLmgaXL7p",
    "outputId": "e1828a00-9e2d-479b-be7f-a13e9e58e9cf"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model_prembert = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lboTePPdXL_j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel,TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC8Yek8_XMF7"
   },
   "outputs": [],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "model_preroberta =TFAutoModel.from_pretrained(\"urduhack/roberta-urdu-small\",from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEZTZNTxZFGe"
   },
   "source": [
    "## **Tokenize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen=df['lemmatized_text'].apply(lambda x: len(x.split()))\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TP5Fdrm2Z9vw"
   },
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBIv0YmOJNIL"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_df1['class'].tolist())\n",
    "\n",
    "y_train = encoder.transform(train_df1['class'].tolist())\n",
    "y_test = encoder.transform(test_df1['class'].tolist())\n",
    "y_val = encoder.transform(val_df1['class'].tolist())\n",
    "\n",
    "#y_train = y_train.reshape(-1,1)\n",
    "#y_test = y_test.reshape(-1,1)\n",
    "#y_val = y_val.reshape(-1,1)\n",
    "\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_test\",y_test.shape)\n",
    "print(\"y_val\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = y_train.reshape(-1,1)\n",
    "y_test_1 = y_test.reshape(-1,1)\n",
    "y_val_1 = y_val.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels = positive:2,neutral:1,negative:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXgDJNMwHSlE"
   },
   "outputs": [],
   "source": [
    "def bert_encode(data):\n",
    "    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    \n",
    "    return tokens['input_ids'],tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robert_encode(data):\n",
    "    tokens1 = tokenizer_roberta.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    \n",
    "    return(tokens1['input_ids'], tokens1[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_1=train_df1.lemmatized_text.tolist()\n",
    "val_encoded_1=val_df1.lemmatized_text.tolist()\n",
    "train_encoded_1 = robert_encode(train_encoded_1)\n",
    "val_encoded_1 = robert_encode(val_encoded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_encoded_1, y_train_1))\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size,drop_remainder=True)\n",
    ")\n",
    "\n",
    "val_dataset_1 = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((val_encoded_1, y_val_1))\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size,drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzOSBo4XTybj"
   },
   "outputs": [],
   "source": [
    "train_encoded = bert_encode(train_df1.lemmatized_text)\n",
    "val_encoded = bert_encode(val_df1.lemmatized_text)\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_encoded, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size,drop_remainder=True)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((val_encoded, y_val))\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size,drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnWJmROYPj9I"
   },
   "source": [
    "## **Building Model Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtbxfEB32jcZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def bert_model(selected_model):\n",
    "\n",
    "    bert_encoder = (selected_model)\n",
    "    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "    mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_masks\")\n",
    "    last_hidden_states = bert_encoder(input_word_ids,mask)[0]\n",
    "    x = tf.keras.layers.Conv1D(16,2,activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.L2(l2=0.0005))(last_hidden_states)\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv1D(32,2,activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.L2(l2=0.0005))(x)\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x= tf.keras.layers.GlobalMaxPooling1D()(x) \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(3, activation='softmax',kernel_regularizer=tf.keras.regularizers.L2(l2=1e-6))(x)\n",
    "    model = tf.keras.Model(inputs=[input_word_ids,mask],outputs=outputs)\n",
    "    model.layers[2].trainable=False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_model(model_prembert)\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "acc=tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name=\"accuracy\")\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False,name=\"sparse_categorical_crossentropy\"),\n",
    "              optimizer=adam_optimizer,metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "BgI6IuCca3EG",
    "outputId": "e0ead9af-9a35-4ae6-ecc6-67ccfba63ff9"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUuYPdX1iAce"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"max\",\n",
    "    cooldown=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1s5tfW_xiE29",
    "outputId": "af9f02bc-b29b-40dd-849d-620286bb0157",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "         train_dataset,\n",
    "         batch_size=batch_size,\n",
    "         epochs=50,\n",
    "         validation_data=val_dataset,\n",
    "         callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10,7))\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "plt.title(label='Bert-Multilingual Urdu',loc='center')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('Bert_model/Bert_model.h5') is False:\n",
    "     model.save('Bert_model/Bert_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = bert_encode(test_df1.lemmatized_text)\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((test_encoded,y_test))\n",
    "    .batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======test dataset ====')\n",
    "for element in test_dataset.as_numpy_iterator(): \n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "predicted_labels = model.predict(test_dataset, batch_size=batch_size)\n",
    "predicted_labels_sparse_categorical = tf.cast(tf.round(predicted_labels), tf.int32).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_sparse_categorical = tf.cast(tf.round(predicted_labels), tf.int32).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels = positive:2,neutral:1,negative:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction\")\n",
    "result_pred = model.predict(test_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.argmax(result_pred, axis=-1)\n",
    "classes=classes.reshape(-1,1)\n",
    "df_actual = pd.DataFrame(y_test, columns = ['actual'])\n",
    "df_predicted_bert = pd.DataFrame(classes, columns = ['predicted'])\n",
    "final_bert=pd.concat([df_actual, df_predicted_bert],axis=1)\n",
    "test_pred_output_bert=pd.concat([test_df1['lemmatized_text'].reset_index(drop=True), final_bert],axis=1)\n",
    "test_pred_output_bert.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Negative', 'Neutral', 'Positive']\n",
    "print(classification_report(y_test, classes,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\".0%\", cmap=\"Blues\",ax=ax)\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, classes)\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(cmn, index=target_names, columns=target_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, classes, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Daq-tbOWKS9"
   },
   "source": [
    "## **Roberta-Urdu Small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oY4BZgP6V3nM"
   },
   "outputs": [],
   "source": [
    "model_roberta = bert_model(model_preroberta)\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "acc=tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name=\"accuracy\")\n",
    "\n",
    "model_roberta.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False,name=\"sparse_categorical_crossentropy\"),\n",
    "              optimizer=adam_optimizer,metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOGZ97mTV3pS"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_roberta, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AWjY8doV3vn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history_roberta = model_roberta.fit(\n",
    "    train_dataset_1,\n",
    "    batch_size=batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_1,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('Roberta_model/Roberta_model.h5') is False:\n",
    "     model_roberta.save('Roberta_model/Roberta_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WuvgkTYV32T"
   },
   "outputs": [],
   "source": [
    "test_encoded_1=test_df1.lemmatized_text.tolist()\n",
    "test_encoded_1 = robert_encode(test_encoded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqwoDu7pV3x8"
   },
   "outputs": [],
   "source": [
    "test_dataset_1 = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((test_encoded_1,y_test_1))\n",
    "    .batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeTA6Rm2V34j"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate\")\n",
    "result_1 = model_roberta.evaluate(test_dataset_1, batch_size=batch_size)\n",
    "dict(zip(model_roberta.metrics_names, result_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY:\",result_1[1])\n",
    "print(\"LOSS:\",result_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction\")\n",
    "result_pred_roberta = model_roberta.predict(test_dataset_1, batch_size=batch_size)\n",
    "#dict(zip(model.metrics_names, result_pred_pred_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(model.metrics_names, result_pred_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pred_roberta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10,7))\n",
    "ax[0].plot(history_roberta.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history_roberta.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "ax[1].plot(history_roberta.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history_roberta.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "plt.title(label='Roberta Urdu',loc='center')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels = positive:2,neutral:1,negative:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_1 = np.argmax(result_pred_roberta, axis=-1)\n",
    "classes_1=classes_1.reshape(-1,1)\n",
    "df_actual = pd.DataFrame(y_test, columns = ['actual'])\n",
    "df_predicted = pd.DataFrame(classes_1, columns = ['predicted'])\n",
    "df_predicted.head(5)\n",
    "final=pd.concat([df_actual, df_predicted],axis=1)\n",
    "test_pred_output=pd.concat([test_df1['lemmatized_text'].reset_index(drop=True), final],axis=1)\n",
    "test_pred_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Negative', 'Neutral', 'Positive']\n",
    "print(classification_report(y_test, classes_1,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_1 = confusion_matrix(y_test, classes_1)\n",
    "cmn_1 = cm_1.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm_1 = pd.DataFrame(cmn_1, index=target_names, columns=target_names)\n",
    "show_confusion_matrix(df_cm_1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "GPU-TF2",
   "language": "python",
   "name": "gpu-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
