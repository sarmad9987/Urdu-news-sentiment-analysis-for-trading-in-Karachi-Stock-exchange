{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed2b4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel,TFRobertaModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel,TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e972e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72efcc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "Bert_model = tf.keras.models.load_model('Bert_model/Bert_model.h5',\n",
    "                                        custom_objects={'TFBertModel':TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96f6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  177853440   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 39, 16)       24592       ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 39, 16)      64          ['conv1d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 38, 32)       1056        ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 38, 32)      128         ['conv1d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_27 (Globa  (None, 32)          0           ['batch_normalization_42[0][0]'] \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_26 (Flatten)           (None, 32)           0           ['global_max_pooling1d_27[0][0]']\n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 3)            99          ['flatten_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177,879,379\n",
      "Trainable params: 177,879,283\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a339f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "Bert_sampled = tf.keras.models.load_model('Bert_model_sampled/Bert_model_sampled.h5',\n",
    "                                          custom_objects={'TFBertModel':TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87fb884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  177853440   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 39, 16)       24592       ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 39, 16)      64          ['conv1d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 38, 32)       1056        ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 38, 32)      128         ['conv1d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_27 (Globa  (None, 32)          0           ['batch_normalization_42[0][0]'] \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_26 (Flatten)           (None, 32)           0           ['global_max_pooling1d_27[0][0]']\n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 3)            99          ['flatten_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177,879,379\n",
      "Trainable params: 177,879,283\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1baa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "Roberta_model = tf.keras.models.load_model('Roberta_model/Roberta_model.h5',\n",
    "                                           custom_objects={'TFRobertaModel': TFRobertaModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "811d483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  125978112  ['input_ids[0][0]',              \n",
      " el)                            thPoolingAndCrossAt               'attention_masks[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)             (None, 39, 16)       24592       ['tf_roberta_model[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 39, 16)      64          ['conv1d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_48 (Conv1D)             (None, 38, 32)       1056        ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 38, 32)      128         ['conv1d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_29 (Globa  (None, 32)          0           ['batch_normalization_46[0][0]'] \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_28 (Flatten)           (None, 32)           0           ['global_max_pooling1d_29[0][0]']\n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 3)            99          ['flatten_28[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 126,004,051\n",
      "Trainable params: 126,003,955\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Roberta_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3c32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "Roberta_model_sampled = tf.keras.models.load_model('Roberta_sampled/Roberta_sampled.h5',\n",
    "                                                   custom_objects={'TFRobertaModel': TFRobertaModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d03122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  125978112  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'attention_masks[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " conv1d_53 (Conv1D)             (None, 39, 16)       24592       ['tf_roberta_model_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 39, 16)      64          ['conv1d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_54 (Conv1D)             (None, 38, 32)       1056        ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 38, 32)      128         ['conv1d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_32 (Globa  (None, 32)          0           ['batch_normalization_52[0][0]'] \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_31 (Flatten)           (None, 32)           0           ['global_max_pooling1d_32[0][0]']\n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3)            99          ['flatten_31[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 126,004,051\n",
      "Trainable params: 126,003,955\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Roberta_model_sampled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab1974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU-TF2",
   "language": "python",
   "name": "gpu-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
